{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e86cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T14:04:19.757736Z",
     "iopub.status.busy": "2025-11-03T14:04:19.757551Z",
     "iopub.status.idle": "2025-11-03T14:05:18.396127Z",
     "shell.execute_reply": "2025-11-03T14:05:18.395487Z"
    },
    "papermill": {
     "duration": 58.643448,
     "end_time": "2025-11-03T14:05:18.397371",
     "exception": false,
     "start_time": "2025-11-03T14:04:19.753923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:04:21.455915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762178661.696676      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762178661.763700      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Starting Training for 10x10 vs 400x400 ==================\n",
      "Found potential Reynolds numbers in file: [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150]\n",
      "\n",
      "Training on 42 samples (from Re: [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700])\n",
      "Testing on 27 samples (from Re: [750, 800, 850, 900, 950, 1000, 1050, 1100, 1150])\n",
      "\n",
      "ðŸ“Š Computing component-specific statistics...\n",
      "  LR Stats - U: mean=0.000121, std=0.166170\n",
      "  LR Stats - V: mean=0.000074, std=0.150482\n",
      "  LR Stats - P: mean=-0.014871, std=0.046746\n",
      "  HR Stats - U: mean=0.000003, std=0.214580\n",
      "  HR Stats - V: mean=0.000000, std=0.181692\n",
      "  HR Stats - P: mean=-0.016368, std=0.067666\n",
      "Building model components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762178677.949294      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1762178677.950051      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models built successfully for LR=10 and HR=400.\n",
      "\n",
      "ðŸš€ Starting Super-Resolution AE training (10x10 -> 400x400)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762178683.838135      64 service.cc:148] XLA service 0x787670002840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762178683.839276      64 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762178683.839295      64 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762178684.352600      64 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1762178688.912855      64 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training finished.\n",
      "âœ… Starting evaluation loop...\n",
      "\n",
      "â–¶ï¸ Evaluating for Re=1000\n",
      "  - Component 'U': MAE=0.0100, NMAE=0.73%\n",
      "  - Component 'V': MAE=0.0110, NMAE=1.04%\n",
      "  - Component 'P': MAE=0.0029, NMAE=0.13%\n",
      "\n",
      "ðŸ“Š Average Metrics for 10x10 -> 400x400 Super-Resolution:\n",
      "  - Average MAE: 0.0080\n",
      "  - Average NMAE: 0.63%\n",
      "\n",
      "ðŸ’¾ Saving models and component-specific stats...\n",
      "âœ… Done for 10x10 vs 400x400.\n"
     ]
    }
   ],
   "source": [
    "# (Keep all imports and helper functions the same)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import h5py\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# ========================= MODIFICATION: MULTI-FILE DATA LOADING WITH BC TYPE TRACKING =========================\n",
    "def load_paired_reynolds_multi(file_paths, lr_dim, hr_dim):\n",
    "    \"\"\"\n",
    "    Loads and concatenates data from multiple HDF5 files (e.g., different BC conditions).\n",
    "    Tracks BC type for each sample for potential filtering/analysis.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of HDF5 file paths to load from\n",
    "        lr_dim: Low-resolution mesh dimension\n",
    "        hr_dim: High-resolution mesh dimension\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (x_lr, x_hr, reynolds_numbers, components, bc_types)\n",
    "    \"\"\"\n",
    "    xs_lr_all, xs_hr_all, used_res_all, components_all, bc_types_all = [], [], [], [], []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\nðŸ“‚ Loading from: {file_path}\")\n",
    "        xs_lr, xs_hr, used_res, components, bc_types = [], [], [], [], []\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                all_keys = list(f.keys())\n",
    "                if not all_keys: \n",
    "                    print(f\"âš ï¸  File is empty, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                re_numbers_in_file = sorted(list(set(int(k.split('_')[0][2:]) for k in all_keys if k.startswith(\"Re\"))))\n",
    "                print(f\"   Found Reynolds numbers: {re_numbers_in_file}\")\n",
    "                \n",
    "                # Try to detect BC type from first group's attributes\n",
    "                first_group = f[all_keys[0]]\n",
    "                bc_type = first_group.attrs.get('bc_type', 'unknown')\n",
    "                print(f\"   BC Type: {bc_type}\")\n",
    "                \n",
    "                for Re in re_numbers_in_file:\n",
    "                    g_lr = f\"Re{Re}_mesh{lr_dim}x{lr_dim}\"\n",
    "                    g_hr = f\"Re{Re}_mesh{hr_dim}x{hr_dim}\"\n",
    "                    \n",
    "                    if g_lr in all_keys and g_hr in all_keys:\n",
    "                        # For each component, load the data\n",
    "                        for comp in ['u', 'v', 'p']:\n",
    "                            if comp in f[g_lr] and comp in f[g_hr]:\n",
    "                                xs_lr.append(f[g_lr][comp][()].astype(np.float32).reshape(lr_dim, lr_dim))\n",
    "                                xs_hr.append(f[g_hr][comp][()].astype(np.float32).reshape(hr_dim, hr_dim))\n",
    "                                used_res.append(Re)\n",
    "                                components.append(comp)\n",
    "                                bc_types.append(bc_type)\n",
    "                \n",
    "                print(f\"   âœ… Loaded {len(xs_lr)} samples from this file\")\n",
    "                \n",
    "        except (IOError, OSError, FileNotFoundError) as e:\n",
    "            print(f\"âš ï¸  Error opening file: {e}\")\n",
    "            print(f\"   Skipping this file.\")\n",
    "            continue\n",
    "        \n",
    "        # Accumulate data from this file\n",
    "        xs_lr_all.extend(xs_lr)\n",
    "        xs_hr_all.extend(xs_hr)\n",
    "        used_res_all.extend(used_res)\n",
    "        components_all.extend(components)\n",
    "        bc_types_all.extend(bc_types)\n",
    "    \n",
    "    if len(xs_lr_all) == 0:\n",
    "        print(\"\\nâš ï¸ No data loaded from any file. Creating dummy data...\")\n",
    "        N = 20\n",
    "        factor = hr_dim // lr_dim\n",
    "        if hr_dim % lr_dim != 0: raise ValueError(\"For dummy data, hr_dim must be a multiple of lr_dim.\")\n",
    "        \n",
    "        for comp in ['u', 'v', 'p']:\n",
    "            x_hr_comp = np.random.randn(N, hr_dim, hr_dim, 1).astype(np.float32)\n",
    "            x_lr_comp = tf.nn.avg_pool(x_hr_comp, ksize=factor, strides=factor, padding='VALID').numpy()\n",
    "            xs_lr_all.extend(x_lr_comp)\n",
    "            xs_hr_all.extend(x_hr_comp)\n",
    "            used_res_all.extend(np.arange(50, 50*N+1, 50))\n",
    "            components_all.extend([comp] * N)\n",
    "            bc_types_all.extend(['dummy'] * N)\n",
    "        \n",
    "        return (np.array(xs_lr_all, dtype=np.float32), \n",
    "                np.array(xs_hr_all, dtype=np.float32), \n",
    "                np.array(used_res_all), \n",
    "                np.array(components_all),\n",
    "                np.array(bc_types_all))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Total loaded: {len(xs_lr_all)} samples from {len(file_paths)} file(s)\")\n",
    "    print(f\"   BC Type distribution: {dict(zip(*np.unique(bc_types_all, return_counts=True)))}\")\n",
    "    \n",
    "    # âš ï¸ DATA BALANCE CHECK - Show Reynolds number coverage per BC type\n",
    "    print(f\"\\nðŸ“‹ Reynolds Number Coverage by BC Type:\")\n",
    "    used_res_array = np.array(used_res_all)\n",
    "    bc_types_array = np.array(bc_types_all)\n",
    "    for bc in np.unique(bc_types_array):\n",
    "        bc_mask = bc_types_array == bc\n",
    "        re_in_bc = np.unique(used_res_array[bc_mask])\n",
    "        print(f\"   {bc}: {sorted(re_in_bc.tolist())}\")\n",
    "    \n",
    "    return (np.array(xs_lr_all, dtype=np.float32)[..., None], \n",
    "            np.array(xs_hr_all, dtype=np.float32)[..., None], \n",
    "            np.array(used_res_all), \n",
    "            np.array(components_all),\n",
    "            np.array(bc_types_all))\n",
    "\n",
    "def dataset_standardize(arr): mean, std = np.mean(arr, dtype=np.float64), np.std(arr, dtype=np.float64); std=1e-8 if std==0 else std; return (arr-mean)/std, float(mean), float(std)\n",
    "def standardize_with_stats(arr, mean, std): std=1e-8 if std==0 else std; return (arr-mean)/std\n",
    "def inverse_standardize(arr, mean, std): return arr*std+mean\n",
    "\n",
    "# ----------------------\n",
    "# PLOTTING FUNCTION\n",
    "# ----------------------\n",
    "def plot_superres_comparison(low_res_true, high_res_true, high_res_pred, reynolds_num, component, lr_dims, hr_dims, mae_value, nmae_percentage):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive plot with INDIVIDUAL color bars for each subplot.\n",
    "    Now includes MAE and NMAE in the difference plot title and component info.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    gs = gridspec.GridSpec(2, 3, figure=fig, height_ratios=[1, 1])\n",
    "    ax0, ax1, ax2 = fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[0, 2])\n",
    "    ax3 = fig.add_subplot(gs[1, :]) # Difference plot spans the bottom row\n",
    "    \n",
    "    cmap = \"RdBu\"\n",
    "    \n",
    "    im0 = ax0.contourf(low_res_true, levels=20, cmap=cmap)\n",
    "    fig.colorbar(im0, ax=ax0).set_label(\"Field Value\")\n",
    "    ax0.set_title(f\"Ground Truth ({lr_dims[1]}x{lr_dims[0]})\")\n",
    "    ax0.set_aspect('equal')\n",
    "\n",
    "    im1 = ax1.contourf(high_res_true, levels=20, cmap=cmap)\n",
    "    fig.colorbar(im1, ax=ax1).set_label(\"Field Value\")\n",
    "    ax1.set_title(f\"Ground Truth ({hr_dims[1]}x{hr_dims[0]})\")\n",
    "    ax1.set_aspect('equal')\n",
    "\n",
    "    im2 = ax2.contourf(high_res_pred, levels=20, cmap=cmap)\n",
    "    fig.colorbar(im2, ax=ax2).set_label(\"Field Value\")\n",
    "    ax2.set_title(f\"Super-Resolved Prediction ({hr_dims[1]}x{hr_dims[0]})\")\n",
    "    ax2.set_aspect('equal')\n",
    "\n",
    "    diff = high_res_true - high_res_pred\n",
    "    diff_max_abs = np.abs(diff).max()\n",
    "    im3 = ax3.contourf(diff, levels=20, cmap=cmap, vmin=-diff_max_abs, vmax=diff_max_abs)\n",
    "    fig.colorbar(im3, ax=ax3).set_label(\"Error\")\n",
    "    \n",
    "    ax3.set_title(f\"Difference (Error) | MAE: {mae_value:.4f} | NMAE: {nmae_percentage:.2f}%\")\n",
    "    ax3.set_aspect('equal')\n",
    "    \n",
    "    fig.suptitle(f\"Super-Resolution for Re={reynolds_num}, Component='{component.upper()}'\", fontsize=16)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# ARCHITECTURE WITH Swish ACTIVATION\n",
    "# ===================================================================================\n",
    "def build_encoder_10(latent_dim=50):\n",
    "    inp = layers.Input(shape=(10, 10, 1), name='encoder_10_input')\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_10')\n",
    "\n",
    "def build_decoder_10(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_10_input')\n",
    "    x = layers.Dense(5 * 5 * 128, activation='swish')(inp)\n",
    "    x = layers.Reshape((5, 5, 128))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_10')(x)\n",
    "    return Model(inp, output, name='decoder_10')\n",
    "\n",
    "def build_encoder_80(latent_dim=50):\n",
    "    inp = layers.Input(shape=(80, 80, 1), name='encoder_80_input')\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_80')\n",
    "\n",
    "def build_decoder_80(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_80_input')\n",
    "    x = layers.Dense(5 * 5 * 256, activation='swish')(inp)\n",
    "    x = layers.Reshape((5, 5, 256))(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(16, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_80')(x)\n",
    "    return Model(inp, output, name='decoder_80')\n",
    "\n",
    "def build_encoder_20(latent_dim=50):\n",
    "    inp = layers.Input(shape=(20, 20, 1), name='encoder_20_input')\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_20')\n",
    "\n",
    "def build_decoder_20(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_20_input')\n",
    "    x = layers.Dense(5 * 5 * 128, activation='swish')(inp)\n",
    "    x = layers.Reshape((5, 5, 128))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_20')(x)\n",
    "    return Model(inp, output, name='decoder_20')\n",
    "\n",
    "def build_encoder_50(latent_dim=50):\n",
    "    inp = layers.Input(shape=(50, 50, 1), name='encoder_50_input')\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(512, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_50')\n",
    "\n",
    "def build_decoder_50(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_50_input')\n",
    "    x = layers.Dense(3 * 3 * 512, activation='swish')(inp)\n",
    "    x = layers.Reshape((3, 3, 512))(x)\n",
    "    x = layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_50')(x)\n",
    "    return Model(inp, output, name='decoder_50')\n",
    "\n",
    "def build_encoder_100(latent_dim=50):\n",
    "    inp = layers.Input(shape=(100, 100, 1), name='encoder_100_input')\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(512, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_100')\n",
    "\n",
    "def build_decoder_100(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_100_input')\n",
    "    x = layers.Dense(3 * 3 * 512, activation='swish')(inp)\n",
    "    x = layers.Reshape((3, 3, 512))(x)\n",
    "    x = layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_100')(x)\n",
    "    return Model(inp, output, name='decoder_100')\n",
    "\n",
    "def build_encoder_400(latent_dim=50):\n",
    "    inp = layers.Input(shape=(400, 400, 1), name='encoder_400_input')\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same', activation='swish')(inp)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same', activation='swish')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='swish')(x)\n",
    "    z = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "    return Model(inp, z, name='encoder_400')\n",
    "\n",
    "def build_decoder_400(latent_dim=50):\n",
    "    inp = layers.Input(shape=(latent_dim,), name='decoder_400_input')\n",
    "    x = layers.Dense(12 * 12 * 256, activation='swish')(inp)\n",
    "    x = layers.Reshape((12, 12, 256))(x)\n",
    "    x = layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    x = layers.Conv2DTranspose(8, (2,2), strides=(2,2), padding='valid', activation='swish')(x)\n",
    "    output = layers.Conv2D(1, 3, padding='same', name='output_image_400')(x)\n",
    "    return Model(inp, output, name='decoder_400')\n",
    "\n",
    "class SuperResolutionAE(Model):\n",
    "    def __init__(self, encoder_lr, decoder_hr, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_lr = encoder_lr\n",
    "        self.decoder_hr = decoder_hr\n",
    "        self.recon_loss_tracker = tf.keras.metrics.Mean(name=\"recon_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.recon_loss_tracker]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs are the low-resolution images\n",
    "        z = self.encoder_lr(inputs, training=training)\n",
    "        recon_hr = self.decoder_hr(z, training=training)\n",
    "        return recon_hr\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Data is expected to be a tuple of (low_res_images, high_res_images)\n",
    "        x_lr, x_hr = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # The model takes LR images and outputs HR images\n",
    "            pred_hr = self(x_lr, training=True)\n",
    "            # The loss is between the predicted HR image and the true HR image\n",
    "            recon_loss = tf.reduce_mean(tf.keras.losses.mse(x_hr, pred_hr))\n",
    "\n",
    "        grads = tape.gradient(recon_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        self.recon_loss_tracker.update_state(recon_loss)\n",
    "        return {\"recon_loss\": self.recon_loss_tracker.result()}\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_for_re(re, model, x_lr_test, x_hr_test, res_test, comps_test, stats_lr, stats_hr, lr_dim, hr_dim, plot=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model for a given Reynolds number, calculates metrics for each component,\n",
    "    and optionally plots the results.\n",
    "    \"\"\"\n",
    "    # Find all indices for the given Reynolds number\n",
    "    re_indices = np.where(res_test == re)[0]\n",
    "    \n",
    "    if len(re_indices) == 0: \n",
    "        print(f\"âŒ Error: Re={re} not in test set.\")\n",
    "        return [], []\n",
    "        \n",
    "    print(f\"\\nâ–¶ï¸ Evaluating for Re={re}\")\n",
    "    \n",
    "    # stats_lr and stats_hr are now dictionaries with component-specific stats\n",
    "    maes, nmaes = [], []\n",
    "\n",
    "    # Iterate through each component found for this Reynolds number\n",
    "    for idx in re_indices:\n",
    "        component = comps_test[idx]\n",
    "        \n",
    "        # Get component-specific stats\n",
    "        mean_lr, std_lr = stats_lr[component]\n",
    "        mean_hr, std_hr = stats_hr[component]\n",
    "        \n",
    "        x_lr_norm = x_lr_test[idx:idx+1]\n",
    "        pred_hr_norm = model.predict(x_lr_norm, verbose=0)[0,...,0]\n",
    "        true_hr_norm = x_hr_test[idx,...,0]\n",
    "        true_lr_norm = x_lr_test[idx,...,0]\n",
    "        \n",
    "        pred_hr_real = inverse_standardize(pred_hr_norm, mean_hr, std_hr)\n",
    "        true_hr_real = inverse_standardize(true_hr_norm, mean_hr, std_hr)\n",
    "        true_lr_real = inverse_standardize(true_lr_norm, mean_lr, std_lr)\n",
    "\n",
    "        mae = np.mean(np.abs(true_hr_real - pred_hr_real))\n",
    "        data_range = np.max(true_hr_real) - np.min(true_hr_real)\n",
    "        nmae_percentage = (mae / (data_range + 1e-8)) * 100\n",
    "        \n",
    "        maes.append(mae)\n",
    "        nmaes.append(nmae_percentage)\n",
    "        \n",
    "        print(f\"  - Component '{component.upper()}': MAE={mae:.4f}, NMAE={nmae_percentage:.2f}%\")\n",
    "\n",
    "        if plot:\n",
    "            plot_superres_comparison(true_lr_real, true_hr_real, pred_hr_real, int(re), component, (lr_dim, lr_dim), (hr_dim, hr_dim), mae, nmae_percentage)\n",
    "            \n",
    "    return maes, nmaes\n",
    "\n",
    "# ----------------------\n",
    "# Main Training and Evaluation Script\n",
    "# ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # A list of high-resolution dimensions to train against the low-resolution one.\n",
    "    HR_DIMS_TO_TRAIN = [400] \n",
    "\n",
    "    for hr_dim_current in HR_DIMS_TO_TRAIN:\n",
    "        print(f\"================== Starting Training for 10x10 vs {hr_dim_current}x{hr_dim_current} ==================\")\n",
    "        # ========================= USER CONTROL PANEL =========================\n",
    "        # --- Grid Dimensions ---\n",
    "        LR_DIM = 10  # Low-resolution grid size\n",
    "        HR_DIM = hr_dim_current  # High-resolution grid size\n",
    "\n",
    "        # --- Training Hyperparameters ---\n",
    "        EPOCHS = 300\n",
    "        LATENT_DIM = 50\n",
    "        BATCH_SIZE = 8\n",
    "        PLOTTING_ENABLED = True\n",
    "        other_details= \"swish_trained_upto_700_multiBC\"\n",
    "        # ======================================================================\n",
    "\n",
    "        # ========================= MULTI-FILE DATA LOADING =========================\n",
    "        # Define paths to all simulation files (update these paths as needed)\n",
    "        file_paths = [\n",
    "            \"/kaggle/input/re-50-1000-mesh-10-400-ch-5/simulation_result.h5\",           # Single lid BC\n",
    "            \"/kaggle/input/re-50-1000-mesh-10-400-ch-5/simulation_result_double_lid.h5\"  # Double lid BC\n",
    "        ]\n",
    "        \n",
    "        # ========================= PER-FILE REYNOLDS NUMBER CONFIGURATION =========================\n",
    "        # ðŸŽ¯ MANUAL CONTROL: Specify which Re numbers to use for each file\n",
    "        # This allows fine-grained control over data splitting per BC condition\n",
    "        \n",
    "        reynolds_config = {\n",
    "            # For single lid BC file\n",
    "            \"single_lid(u_top=1)\": {\n",
    "                \"train\": [100, 200, 300, 400, 500, 600, 700],\n",
    "                \"test\": [800],\n",
    "                \"evaluate\": [800]\n",
    "            },\n",
    "            # For double lid BC file\n",
    "            \"double_lid(u_top=1,u_bottom=1)\": {\n",
    "                \"train\": [100, 200, 300, 400, 500, 600, 700],  # Example: fewer Re numbers\n",
    "                \"test\": [800],\n",
    "                \"evaluate\": [800]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Alternative: Use 'ALL' to include all available Re numbers for a file\n",
    "        # reynolds_config = {\n",
    "        #     \"single_lid(u_top=1)\": {\n",
    "        #         \"train\": \"ALL\",  # Will use all available Re in the file\n",
    "        #         \"test\": [],\n",
    "        #         \"evaluate\": [1000]\n",
    "        #     },\n",
    "        #     ...\n",
    "        # }\n",
    "        # ========================================================================================\n",
    "        \n",
    "        # Load and concatenate data from all files\n",
    "        x_lr_raw_full, x_hr_raw_full, used_res_full, comps_full, bc_types_full = load_paired_reynolds_multi(file_paths, LR_DIM, HR_DIM)\n",
    "        \n",
    "        # Apply per-file Reynolds number filtering\n",
    "        print(\"\\nðŸŽ¯ Applying per-file Reynolds number configuration...\")\n",
    "        train_mask = np.zeros(len(used_res_full), dtype=bool)\n",
    "        test_mask = np.zeros(len(used_res_full), dtype=bool)\n",
    "        \n",
    "        for bc_type, config in reynolds_config.items():\n",
    "            bc_mask = bc_types_full == bc_type\n",
    "            \n",
    "            # Handle train Reynolds\n",
    "            if config[\"train\"] == \"ALL\":\n",
    "                train_re_for_bc = np.unique(used_res_full[bc_mask])\n",
    "            else:\n",
    "                train_re_for_bc = config[\"train\"]\n",
    "            \n",
    "            # Handle test Reynolds\n",
    "            if config[\"test\"] == \"ALL\":\n",
    "                test_re_for_bc = np.unique(used_res_full[bc_mask])\n",
    "            else:\n",
    "                test_re_for_bc = config[\"test\"]\n",
    "            \n",
    "            # Create masks for this BC type\n",
    "            bc_train_mask = bc_mask & np.isin(used_res_full, train_re_for_bc)\n",
    "            bc_test_mask = bc_mask & np.isin(used_res_full, test_re_for_bc)\n",
    "            \n",
    "            # Accumulate into global masks\n",
    "            train_mask |= bc_train_mask\n",
    "            test_mask |= bc_test_mask\n",
    "            \n",
    "            print(f\"  {bc_type}:\")\n",
    "            print(f\"    Train Re: {sorted(train_re_for_bc)}\")\n",
    "            print(f\"    Test Re:  {sorted(test_re_for_bc)}\")\n",
    "            print(f\"    Train samples: {np.sum(bc_train_mask)}\")\n",
    "            print(f\"    Test samples:  {np.sum(bc_test_mask)}\")\n",
    "        \n",
    "        # Extract train/test data\n",
    "        x_lr_train_raw, x_hr_train_raw, res_train, comps_train = x_lr_raw_full[train_mask], x_hr_raw_full[train_mask], used_res_full[train_mask], comps_full[train_mask]\n",
    "        x_lr_test_raw, x_hr_test_raw, res_test, comps_test = x_lr_raw_full[test_mask], x_hr_raw_full[test_mask], used_res_full[test_mask], comps_full[test_mask]\n",
    "\n",
    "        if len(res_train)==0 or len(res_test)==0: \n",
    "            print(f\"Train or test set is empty for HR_DIM={HR_DIM}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Final Dataset Split:\")\n",
    "        print(f\"  Training: {len(res_train)} samples (Re: {sorted(np.unique(res_train).tolist())})\")\n",
    "        print(f\"  Testing:  {len(res_test)} samples (Re: {sorted(np.unique(res_test).tolist())})\")\n",
    "        \n",
    "        # Collect all evaluation Re numbers from all BC types\n",
    "        reynolds_to_evaluate = []\n",
    "        for bc_type, config in reynolds_config.items():\n",
    "            reynolds_to_evaluate.extend(config[\"evaluate\"])\n",
    "        reynolds_to_evaluate = sorted(list(set(reynolds_to_evaluate)))  # Remove duplicates\n",
    "        print(f\"  Evaluation Re: {reynolds_to_evaluate}\")\n",
    "\n",
    "        # ============== COMPONENT-SPECIFIC STANDARDIZATION ==============\n",
    "        print(\"\\nðŸ“Š Computing component-specific statistics...\")\n",
    "        \n",
    "        # Separate training data by component\n",
    "        u_mask_train = comps_train == 'u'\n",
    "        v_mask_train = comps_train == 'v'\n",
    "        p_mask_train = comps_train == 'p'\n",
    "        \n",
    "        # Compute statistics for each component (LR)\n",
    "        x_lr_u_train, mean_lr_u, std_lr_u = dataset_standardize(x_lr_train_raw[u_mask_train])\n",
    "        x_lr_v_train, mean_lr_v, std_lr_v = dataset_standardize(x_lr_train_raw[v_mask_train])\n",
    "        x_lr_p_train, mean_lr_p, std_lr_p = dataset_standardize(x_lr_train_raw[p_mask_train])\n",
    "        \n",
    "        # Compute statistics for each component (HR)\n",
    "        x_hr_u_train, mean_hr_u, std_hr_u = dataset_standardize(x_hr_train_raw[u_mask_train])\n",
    "        x_hr_v_train, mean_hr_v, std_hr_v = dataset_standardize(x_hr_train_raw[v_mask_train])\n",
    "        x_hr_p_train, mean_hr_p, std_hr_p = dataset_standardize(x_hr_train_raw[p_mask_train])\n",
    "        \n",
    "        print(f\"  LR Stats - U: mean={mean_lr_u:.6f}, std={std_lr_u:.6f}\")\n",
    "        print(f\"  LR Stats - V: mean={mean_lr_v:.6f}, std={std_lr_v:.6f}\")\n",
    "        print(f\"  LR Stats - P: mean={mean_lr_p:.6f}, std={std_lr_p:.6f}\")\n",
    "        print(f\"  HR Stats - U: mean={mean_hr_u:.6f}, std={std_hr_u:.6f}\")\n",
    "        print(f\"  HR Stats - V: mean={mean_hr_v:.6f}, std={std_hr_v:.6f}\")\n",
    "        print(f\"  HR Stats - P: mean={mean_hr_p:.6f}, std={std_hr_p:.6f}\")\n",
    "        \n",
    "        # Reconstruct full training arrays with component-specific standardization\n",
    "        x_lr_train = np.zeros_like(x_lr_train_raw)\n",
    "        x_hr_train = np.zeros_like(x_hr_train_raw)\n",
    "        \n",
    "        x_lr_train[u_mask_train] = x_lr_u_train\n",
    "        x_lr_train[v_mask_train] = x_lr_v_train\n",
    "        x_lr_train[p_mask_train] = x_lr_p_train\n",
    "        \n",
    "        x_hr_train[u_mask_train] = x_hr_u_train\n",
    "        x_hr_train[v_mask_train] = x_hr_v_train\n",
    "        x_hr_train[p_mask_train] = x_hr_p_train\n",
    "        \n",
    "        # Standardize test data using component-specific stats\n",
    "        u_mask_test = comps_test == 'u'\n",
    "        v_mask_test = comps_test == 'v'\n",
    "        p_mask_test = comps_test == 'p'\n",
    "        \n",
    "        x_lr_test = np.zeros_like(x_lr_test_raw)\n",
    "        x_hr_test = np.zeros_like(x_hr_test_raw)\n",
    "        \n",
    "        x_lr_test[u_mask_test] = standardize_with_stats(x_lr_test_raw[u_mask_test], mean_lr_u, std_lr_u)\n",
    "        x_lr_test[v_mask_test] = standardize_with_stats(x_lr_test_raw[v_mask_test], mean_lr_v, std_lr_v)\n",
    "        x_lr_test[p_mask_test] = standardize_with_stats(x_lr_test_raw[p_mask_test], mean_lr_p, std_lr_p)\n",
    "        \n",
    "        x_hr_test[u_mask_test] = standardize_with_stats(x_hr_test_raw[u_mask_test], mean_hr_u, std_hr_u)\n",
    "        x_hr_test[v_mask_test] = standardize_with_stats(x_hr_test_raw[v_mask_test], mean_hr_v, std_hr_v)\n",
    "        x_hr_test[p_mask_test] = standardize_with_stats(x_hr_test_raw[p_mask_test], mean_hr_p, std_hr_p)\n",
    "        \n",
    "        # Create dictionaries for easy access during evaluation\n",
    "        stats_lr = {'u': (mean_lr_u, std_lr_u), 'v': (mean_lr_v, std_lr_v), 'p': (mean_lr_p, std_lr_p)}\n",
    "        stats_hr = {'u': (mean_hr_u, std_hr_u), 'v': (mean_hr_v, std_hr_v), 'p': (mean_hr_p, std_hr_p)}\n",
    "        # ================================================================\n",
    "\n",
    "        # --- Build model components ---\n",
    "        print(\"Building model components...\")\n",
    "        enc_lr = build_encoder_10(latent_dim=LATENT_DIM)\n",
    "        \n",
    "        # Dynamically select the high-resolution decoder\n",
    "        decoder_builder = globals()[f'build_decoder_{HR_DIM}']\n",
    "        dec_hr = decoder_builder(latent_dim=LATENT_DIM)\n",
    "        print(f\"âœ… Models built successfully for LR={LR_DIM} and HR={HR_DIM}.\")\n",
    "        \n",
    "        # --- Train Super-Resolution Autoencoder ---\n",
    "        print(f\"\\nðŸš€ Starting Super-Resolution AE training ({LR_DIM}x{LR_DIM} -> {HR_DIM}x{HR_DIM})...\")\n",
    "        superres_model = SuperResolutionAE(enc_lr, dec_hr)\n",
    "        superres_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_lr_train, x_hr_train)).shuffle(len(x_lr_train)).batch(BATCH_SIZE)\n",
    "        \n",
    "        superres_model.fit(train_dataset, epochs=EPOCHS, verbose=0)\n",
    "        print(\"âœ… Training finished.\")\n",
    "\n",
    "        # --- Create the final super-resolution model for inference ---\n",
    "        # The training model is already the inference model\n",
    "        \n",
    "        print(\"âœ… Starting evaluation loop...\")\n",
    "        all_maes, all_nmaes = [], []\n",
    "        for re_val in reynolds_to_evaluate:\n",
    "            maes, nmaes = evaluate_for_re(re_val, superres_model, x_lr_test, x_hr_test, res_test, comps_test,\n",
    "                                     stats_lr, stats_hr, LR_DIM, HR_DIM, plot=PLOTTING_ENABLED)\n",
    "            if maes and nmaes:\n",
    "                all_maes.extend(maes)\n",
    "                all_nmaes.extend(nmaes)\n",
    "\n",
    "        if all_maes and all_nmaes:\n",
    "            avg_mae = np.mean(all_maes)\n",
    "            avg_nmae = np.mean(all_nmaes)\n",
    "            print(f\"\\nðŸ“Š Average Metrics for {LR_DIM}x{LR_DIM} -> {HR_DIM}x{HR_DIM} Super-Resolution:\")\n",
    "            print(f\"  - Average MAE: {avg_mae:.4f}\")\n",
    "            print(f\"  - Average NMAE: {avg_nmae:.2f}%\")\n",
    "        \n",
    "        print(\"\\nðŸ’¾ Saving models and component-specific stats...\")\n",
    "        # Use parameters in filenames for clarity\n",
    "        superres_model.encoder_lr.save(f\"vanilla_encoder{LR_DIM}_to_{HR_DIM}_{other_details}.h5\")\n",
    "        superres_model.decoder_hr.save(f\"vanilla_decoder{HR_DIM}_from_{LR_DIM}_{other_details}.h5\")\n",
    "        superres_model.save(f\"superres_{LR_DIM}to{HR_DIM}_vanilla_ae_{other_details}.h5\")\n",
    "        \n",
    "        # Save component-specific statistics\n",
    "        with open(f\"standardization_stats_{LR_DIM}to{HR_DIM}_{other_details}.txt\",\"w\") as f:\n",
    "            f.write(f\"# Component-specific standardization statistics\\n\")\n",
    "            f.write(f\"# Format: mean<resolution>_<component> value\\n\")\n",
    "            f.write(f\"mean{LR_DIM}_u {mean_lr_u}\\n\")\n",
    "            f.write(f\"std{LR_DIM}_u {std_lr_u}\\n\")\n",
    "            f.write(f\"mean{LR_DIM}_v {mean_lr_v}\\n\")\n",
    "            f.write(f\"std{LR_DIM}_v {std_lr_v}\\n\")\n",
    "            f.write(f\"mean{LR_DIM}_p {mean_lr_p}\\n\")\n",
    "            f.write(f\"std{LR_DIM}_p {std_lr_p}\\n\")\n",
    "            f.write(f\"mean{HR_DIM}_u {mean_hr_u}\\n\")\n",
    "            f.write(f\"std{HR_DIM}_u {std_hr_u}\\n\")\n",
    "            f.write(f\"mean{HR_DIM}_v {mean_hr_v}\\n\")\n",
    "            f.write(f\"std{HR_DIM}_v {std_hr_v}\\n\")\n",
    "            f.write(f\"mean{HR_DIM}_p {mean_hr_p}\\n\")\n",
    "            f.write(f\"std{HR_DIM}_p {std_hr_p}\\n\")\n",
    "        print(f\"âœ… Done for {LR_DIM}x{LR_DIM} vs {HR_DIM}x{HR_DIM}.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8114774,
     "sourceId": 12884701,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.39219,
   "end_time": "2025-11-03T14:05:21.752704",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-03T14:04:15.360514",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
